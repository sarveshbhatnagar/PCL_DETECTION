{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24671e2",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4408c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools, re\n",
    "from collections import Counter \n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB, ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import  recall_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c426b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c65fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task2():\n",
    "    temp = open('dontpatronizeme_categories.tsv')\n",
    "    check = temp.read()\n",
    "    tag2id = {'Unbalanced_power_relations':0,'Shallow_solution':1,'Presupposition':2,'Authority_voice':3,'Metaphors':4,\n",
    "              'Compassion':5,'The_poorer_the_merrier':6}\n",
    "    ls = check.split('\\n')[4:]\n",
    "    df = pd.DataFrame([line.split('\\t')  for line in ls if len(line)>0])\n",
    "    df.columns = ['par_id', 'art_id','text' ,'keyword', 'country','start','finish','text_span','label','num_annotators' ]\n",
    "    df['labelid'] = df['label'].map(tag2id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe84463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_cat_df = load_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7444be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Not doing the mlb transformation. since I will use labelid column\n",
    "# # pcl_cat_df.label = [list(i) for i in mlb.fit_transform([{i} for i in pcl_cat_df.label]) ]\n",
    "# pcl_cat_df.labelid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d8f3f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>start</th>\n",
       "      <th>finish</th>\n",
       "      <th>text_span</th>\n",
       "      <th>label</th>\n",
       "      <th>num_annotators</th>\n",
       "      <th>labelid</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_span_split</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>201</td>\n",
       "      <td>236</td>\n",
       "      <td>put their situation in perspective</td>\n",
       "      <td>Authority_voice</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[we, also, know, that, they, can, benefit, by,...</td>\n",
       "      <td>[put, their, situation, in, perspective]</td>\n",
       "      <td>[0.9346621, -5.8665924, -2.6035552, -3.6673388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>240</td>\n",
       "      <td>274</td>\n",
       "      <td>help them communicate with others</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[we, also, know, that, they, can, benefit, by,...</td>\n",
       "      <td>[help, them, communicate, with, others]</td>\n",
       "      <td>[1.0959765, -13.135075, -5.5036626, -7.898024,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>274</td>\n",
       "      <td>300</td>\n",
       "      <td>who could provide support</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[we, also, know, that, they, can, benefit, by,...</td>\n",
       "      <td>[who, could, provide, support]</td>\n",
       "      <td>[-3.9163299, -6.50406, -3.1795444, -10.035988,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>384</td>\n",
       "      <td>434</td>\n",
       "      <td>plan for their needs and the needs of their child</td>\n",
       "      <td>Authority_voice</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[we, also, know, that, they, can, benefit, by,...</td>\n",
       "      <td>[plan, for, their, needs, and, the, needs, of,...</td>\n",
       "      <td>[-0.5853478, -9.880554, 0.27753043, -9.771219,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>We also know that they can benefit</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[we, also, know, that, they, can, benefit, by,...</td>\n",
       "      <td>[we, also, know, that, they, can, benefit]</td>\n",
       "      <td>[5.664587, -4.916066, -14.801041, -7.9291306, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>10070</td>\n",
       "      <td>@@15573661</td>\n",
       "      <td>Fern ? ndez was a well-known philanthropist wh...</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ng</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>Fern ? ndez was a well-known philanthropist wh...</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[fern, ?, ndez, was, a, well-known, philanthro...</td>\n",
       "      <td>[fern, ?, ndez, was, a, well-known, philanthro...</td>\n",
       "      <td>[14.108637, -35.828888, -15.737808, 3.5372179,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>6484</td>\n",
       "      <td>@@2559173</td>\n",
       "      <td>Touched much by their plight , Commanding Offi...</td>\n",
       "      <td>homeless</td>\n",
       "      <td>lk</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>Touched much by their plight</td>\n",
       "      <td>Compassion</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[touched, much, by, their, plight, commanding,...</td>\n",
       "      <td>[touched, much, by, their, plight]</td>\n",
       "      <td>[2.3005762, -5.1412964, -4.6743145, -3.1482363...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>6484</td>\n",
       "      <td>@@2559173</td>\n",
       "      <td>Touched much by their plight , Commanding Offi...</td>\n",
       "      <td>homeless</td>\n",
       "      <td>lk</td>\n",
       "      <td>31</td>\n",
       "      <td>315</td>\n",
       "      <td>Commanding Officer and all ranks of the 7 Sri ...</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[touched, much, by, their, plight, commanding,...</td>\n",
       "      <td>[commanding, officer, and, all, ranks, of, the...</td>\n",
       "      <td>[-29.726833, 5.807111, 11.023658, -2.702696, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>6249</td>\n",
       "      <td>@@1947926</td>\n",
       "      <td>She reiterated her ministry 's commitment to p...</td>\n",
       "      <td>women</td>\n",
       "      <td>gh</td>\n",
       "      <td>153</td>\n",
       "      <td>205</td>\n",
       "      <td>gave a strong indication of hope for Ghanaian ...</td>\n",
       "      <td>Unbalanced_power_relations</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[she, reiterated, her, ministry, s, commitment...</td>\n",
       "      <td>[gave, a, strong, indication, of, hope, for, g...</td>\n",
       "      <td>[2.9761238, -5.6566, -8.207358, 4.8946347, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>5149</td>\n",
       "      <td>@@1789214</td>\n",
       "      <td>Preaching the sermon , the Dean of the St. Pet...</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>gh</td>\n",
       "      <td>23</td>\n",
       "      <td>317</td>\n",
       "      <td>the Dean of the St. Peter 's Cathedral , Very ...</td>\n",
       "      <td>Authority_voice</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[preaching, the, sermon, the, dean, of, the, s...</td>\n",
       "      <td>[the, dean, of, the, st, ., peter, s, cathedra...</td>\n",
       "      <td>[28.13755, -11.874865, -31.337564, 11.80393, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id      art_id                                               text  \\\n",
       "0      4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "1      4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "2      4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "3      4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "4      4046  @@14767805  We also know that they can benefit by receivin...   \n",
       "...     ...         ...                                                ...   \n",
       "2755  10070  @@15573661  Fern ? ndez was a well-known philanthropist wh...   \n",
       "2756   6484   @@2559173  Touched much by their plight , Commanding Offi...   \n",
       "2757   6484   @@2559173  Touched much by their plight , Commanding Offi...   \n",
       "2758   6249   @@1947926  She reiterated her ministry 's commitment to p...   \n",
       "2759   5149   @@1789214  Preaching the sermon , the Dean of the St. Pet...   \n",
       "\n",
       "         keyword country start finish  \\\n",
       "0       hopeless      us   201    236   \n",
       "1       hopeless      us   240    274   \n",
       "2       hopeless      us   274    300   \n",
       "3       hopeless      us   384    434   \n",
       "4       hopeless      us     0     35   \n",
       "...          ...     ...   ...    ...   \n",
       "2755    disabled      ng     0    210   \n",
       "2756    homeless      lk     0     29   \n",
       "2757    homeless      lk    31    315   \n",
       "2758       women      gh   153    205   \n",
       "2759  vulnerable      gh    23    317   \n",
       "\n",
       "                                              text_span  \\\n",
       "0                    put their situation in perspective   \n",
       "1                     help them communicate with others   \n",
       "2                             who could provide support   \n",
       "3     plan for their needs and the needs of their child   \n",
       "4                    We also know that they can benefit   \n",
       "...                                                 ...   \n",
       "2755  Fern ? ndez was a well-known philanthropist wh...   \n",
       "2756                       Touched much by their plight   \n",
       "2757  Commanding Officer and all ranks of the 7 Sri ...   \n",
       "2758  gave a strong indication of hope for Ghanaian ...   \n",
       "2759  the Dean of the St. Peter 's Cathedral , Very ...   \n",
       "\n",
       "                           label num_annotators  labelid  \\\n",
       "0                Authority_voice              2        3   \n",
       "1     Unbalanced_power_relations              2        0   \n",
       "2     Unbalanced_power_relations              1        0   \n",
       "3                Authority_voice              2        3   \n",
       "4     Unbalanced_power_relations              1        0   \n",
       "...                          ...            ...      ...   \n",
       "2755  Unbalanced_power_relations              1        0   \n",
       "2756                  Compassion              1        5   \n",
       "2757  Unbalanced_power_relations              1        0   \n",
       "2758  Unbalanced_power_relations              1        0   \n",
       "2759             Authority_voice              1        3   \n",
       "\n",
       "                                             text_split  \\\n",
       "0     [we, also, know, that, they, can, benefit, by,...   \n",
       "1     [we, also, know, that, they, can, benefit, by,...   \n",
       "2     [we, also, know, that, they, can, benefit, by,...   \n",
       "3     [we, also, know, that, they, can, benefit, by,...   \n",
       "4     [we, also, know, that, they, can, benefit, by,...   \n",
       "...                                                 ...   \n",
       "2755  [fern, ?, ndez, was, a, well-known, philanthro...   \n",
       "2756  [touched, much, by, their, plight, commanding,...   \n",
       "2757  [touched, much, by, their, plight, commanding,...   \n",
       "2758  [she, reiterated, her, ministry, s, commitment...   \n",
       "2759  [preaching, the, sermon, the, dean, of, the, s...   \n",
       "\n",
       "                                        text_span_split  \\\n",
       "0              [put, their, situation, in, perspective]   \n",
       "1               [help, them, communicate, with, others]   \n",
       "2                        [who, could, provide, support]   \n",
       "3     [plan, for, their, needs, and, the, needs, of,...   \n",
       "4            [we, also, know, that, they, can, benefit]   \n",
       "...                                                 ...   \n",
       "2755  [fern, ?, ndez, was, a, well-known, philanthro...   \n",
       "2756                 [touched, much, by, their, plight]   \n",
       "2757  [commanding, officer, and, all, ranks, of, the...   \n",
       "2758  [gave, a, strong, indication, of, hope, for, g...   \n",
       "2759  [the, dean, of, the, st, ., peter, s, cathedra...   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [0.9346621, -5.8665924, -2.6035552, -3.6673388...  \n",
       "1     [1.0959765, -13.135075, -5.5036626, -7.898024,...  \n",
       "2     [-3.9163299, -6.50406, -3.1795444, -10.035988,...  \n",
       "3     [-0.5853478, -9.880554, 0.27753043, -9.771219,...  \n",
       "4     [5.664587, -4.916066, -14.801041, -7.9291306, ...  \n",
       "...                                                 ...  \n",
       "2755  [14.108637, -35.828888, -15.737808, 3.5372179,...  \n",
       "2756  [2.3005762, -5.1412964, -4.6743145, -3.1482363...  \n",
       "2757  [-29.726833, 5.807111, 11.023658, -2.702696, 1...  \n",
       "2758  [2.9761238, -5.6566, -8.207358, 4.8946347, 0.9...  \n",
       "2759  [28.13755, -11.874865, -31.337564, 11.80393, -...  \n",
       "\n",
       "[2760 rows x 14 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcl_cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef6deb",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ef93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_words(text):\n",
    "        \"\"\"\n",
    "        Removes contractions from the text.\n",
    "        \"\"\"\n",
    "        return contractions.fix(text)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Should return a list of words\n",
    "    \"\"\"\n",
    "    text = contract_words(text)\n",
    "    text = text.lower()\n",
    "#     text = text.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "    text = text.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\".\",\" .\") ## added by PAVAN\n",
    "    \n",
    "    ## To capture multiple # feature -- added by PAVAN\n",
    "    if re.search(r'[a-z]+\\#',text) :\n",
    "        tmp_ls = text.split()\n",
    "        text = ' '.join( [re.sub(pattern=r'\\#',repl=' #',string=str(i)) if re.search(r'[a-z]+\\#',str(i)) else i for i in tmp_ls])\n",
    "        \n",
    "    ## To capture # feature -- added by PAVAN\n",
    "    if re.search(r'\\#[a-z]+',text) :\n",
    "        tmp_ls = text.split()\n",
    "        text = ' '.join( [re.sub(pattern=r'\\#',repl='hashtagfea ',string=str(i)) if re.search(r'\\#[a-z]+',str(i)) else i for i in tmp_ls])\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c13677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hashtagfea', 'thedignityproject']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('#thedignityproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ce3775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hashtagfea', 'ichh', 'hashtagfea', 'thedignityproject']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('#ichh#thedignityproject')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d848db",
   "metadata": {},
   "source": [
    "## FEATURE GENERATION - word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53cbca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModelTrainer:\n",
    "    \"\"\"\n",
    "    Word2Vec Features. Basic Process: call train, call load.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences=[], path=\"word2vec.wordvectors\"):\n",
    "        \"\"\"\n",
    "        params: \n",
    "        sentences: list of sentences (default: [])\n",
    "        NOTE sentences should be like this \n",
    "        [[\"i\", \"am\", \"ready\"],[\"some\",\"other\",\"sentence\"]]\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.path = path\n",
    "    \n",
    "    ## UPDATED version -- PAVAN\n",
    "    def train(self, size=100, window=7):\n",
    "        \"\"\"\n",
    "        Trains the model on sentences\n",
    "        params: \n",
    "        size: size of the vector\n",
    "        window: window size\n",
    "        \"\"\"\n",
    "        \n",
    "        model = Word2Vec(sentences=self.sentences, vector_size=size,\n",
    "                         window=window, min_count=1, workers=4)\n",
    "        model.save(\"word2vec_categories.model\")\n",
    "        \n",
    "        model.build_vocab(corpus_iterable=self.sentences, update=True)\n",
    "        \n",
    "        model.train(corpus_iterable=self.sentences,\n",
    "                        total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "#         model.train(sentences=self.sentences,\n",
    "#                     total_examples=model.corpus_count, epochs=30)\n",
    "        word_vectors = model.wv\n",
    "        word_vectors.save(self.path)\n",
    "        return model, word_vectors\n",
    "\n",
    "    def load_trained(self, path=\"\"):\n",
    "        \"\"\"\n",
    "        Loads the trained model\n",
    "        \"\"\"\n",
    "        if(path != \"\"):\n",
    "            load_path = path\n",
    "        else:\n",
    "            load_path = self.path\n",
    "        wv = KeyedVectors.load(load_path, mmap='r')\n",
    "        return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66372ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features\n",
    "def get_bigrams(text_split):\n",
    "    \"\"\"\n",
    "    :param text_split: split text (ref preprocessing).\n",
    "    :return: list of bigrams\n",
    "    \"\"\"\n",
    "    bigrams = [[text_split[i], text_split[i+1]]\n",
    "               for i in range(len(text_split)-1)]\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "def add_vectors(list_of_words, wv):\n",
    "    \"\"\"\n",
    "    Returns sum of word vectors, use on text split (ref preprocessing)\n",
    "    params:\n",
    "    list_of_words: list of words\n",
    "    wv: word2vec model\n",
    "    \"\"\"\n",
    "    return sum([wv[word] for word in list_of_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c629cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gensim.models.word2vec.Word2Vec at 0x1e1575656d0>,\n",
       " <gensim.models.keyedvectors.KeyedVectors at 0x1e157565af0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For creating word2vec model & embeddings -> I am using 'text' column. Since it has all the data\n",
    "pcl_cat_df['text_split'] =pcl_cat_df['text'].apply(preprocess_text)\n",
    "\n",
    "unique_sentences_ls =  pcl_cat_df['text_split'].to_list() \n",
    "# # Train WordVectors. Only run once.\n",
    "Word2VecModelTrainer(sentences=unique_sentences_ls, path=\"word2vec_categories.wordvectors\").train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf9274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained word vectors.\n",
    "wv = Word2VecModelTrainer().load_trained(\"word2vec_categories.wordvectors\")\n",
    "\n",
    "pcl_cat_df['text_span_split'] =pcl_cat_df['text_span'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "## Get the treatment dictionary\n",
    "def get_treatment_map(TEXT_SPAN_SPLIT_SERIES, TEXT_SPLIT_SERIES, EMBED):\n",
    "    def custom_key(str):\n",
    "        return -len(str), str.lower()\n",
    "    text_span_token_ls = list(set( list( itertools.chain.from_iterable(TEXT_SPAN_SPLIT_SERIES) ))) #pcl_cat_df.text_span_split\n",
    "    text_token_ls = list(set( list( itertools.chain.from_iterable( TEXT_SPLIT_SERIES ) ))) #pcl_cat_df.text_split\n",
    "    TOBE_TREATED = [token for token in text_span_token_ls if token not in text_token_ls]\n",
    "\n",
    "    sorted_token_ls = sorted(sorted(EMBED.key_to_index.keys() ), key=custom_key)\n",
    "    EMBED_sorted = {i:EMBED[i] for i in sorted_token_ls}\n",
    "    TREATING_TOKENS_MAP ={i:j for i in TOBE_TREATED for j in EMBED_sorted if re.match(pattern=i,string=j) }\n",
    "    return TREATING_TOKENS_MAP\n",
    "\n",
    "TREATING_TOKENS_MAP = get_treatment_map(pcl_cat_df.text_span_split,pcl_cat_df.text_split , wv)\n",
    "\n",
    "## Applying the Treatment on \"text_span_split\" column\n",
    "pcl_cat_df['text_span_split'] = pcl_cat_df['text_span_split'].apply(lambda x: [TREATING_TOKENS_MAP[i] if i in TREATING_TOKENS_MAP else i for i in x])\n",
    "\n",
    "pcl_cat_df['embeddings'] = pcl_cat_df['text_span_split'].apply(add_vectors, wv=wv) #basic_features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "387fbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 774, 6: 774, 5: 546, 2: 237, 3: 228, 4: 200, 1: 181})\n"
     ]
    }
   ],
   "source": [
    "# define oversampling strategy\n",
    "rus = RandomOverSampler(sampling_strategy='minority',random_state=42)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = rus.fit_resample(X_train, y_train)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9781e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 774, 5: 546, 2: 237, 3: 228, 4: 200, 1: 181, 6: 42})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df0265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69276e95",
   "metadata": {},
   "source": [
    "## MODEL VERSION 1 - word2vec- NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34d464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use 'labelid' column for using CategoricalNB\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pcl_cat_df['embeddings'], pcl_cat_df['labelid'], stratify=pcl_cat_df['labelid'], test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# # define oversampling strategy\n",
    "# rus = RandomUnderSampler(sampling_strategy= 'majority',random_state=42) #\n",
    "# # fit and apply the transform\n",
    "# X_over, y_over = rus.fit_resample(np.array(list(X_train)),np.array(list(y_train)) )\n",
    "\n",
    "# X_train = X_over.copy()\n",
    "# y_train = y_over.copy()\n",
    "\n",
    "# y_train = y_train.sort_values()\n",
    "# SORTED_INDICES= list(y_train.sort_values().index)\n",
    "# X_train = X_train.loc[SORTED_INDICES]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform( np.array(list(X_train)) )\n",
    "X_test = scaler.fit_transform( np.array(list(X_test)) )\n",
    "\n",
    "model_NB = ComplementNB()\n",
    "model_NB.fit(X_train,y_train)\n",
    "pred_NB = model_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fd5210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.64       194\n",
      "           1       0.00      0.00      0.00        46\n",
      "           2       0.14      0.08      0.10        59\n",
      "           3       0.16      0.40      0.22        57\n",
      "           4       0.20      0.04      0.07        50\n",
      "           5       0.48      0.53      0.50       136\n",
      "           6       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.41       552\n",
      "   macro avg       0.23      0.24      0.22       552\n",
      "weighted avg       0.38      0.41      0.39       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true= y_test, y_pred= pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebef2d",
   "metadata": {},
   "source": [
    "### Feature Generation: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "215a45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pcl_cat_df.text_split.apply(lambda x: ' '.join(x) ).drop_duplicates().to_list()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(pcl_cat_df.text_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc148546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_textspan_tfidf = pd.DataFrame(X.toarray(),columns= vectorizer.get_feature_names())\n",
    "pca = PCA(n_components=100,random_state=123)\n",
    "\n",
    "pcl_cat_df['tfidf_embeddings'] = list(  list( pca.fit_transform(pcl_textspan_tfidf) )) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73e11b",
   "metadata": {},
   "source": [
    "## MODEL VERSION 2 - tfidf- NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49808906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       194\n",
      "           1       0.26      0.11      0.15        46\n",
      "           2       0.26      0.10      0.15        59\n",
      "           3       0.19      0.07      0.10        57\n",
      "           4       0.26      0.36      0.30        50\n",
      "           5       0.52      0.38      0.43       136\n",
      "           6       0.02      0.20      0.03        10\n",
      "\n",
      "    accuracy                           0.36       552\n",
      "   macro avg       0.29      0.26      0.25       552\n",
      "weighted avg       0.41      0.36      0.37       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pcl_cat_df['tfidf_embeddings'], pcl_cat_df['labelid'], stratify=pcl_cat_df['labelid'], test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# define oversampling strategy\n",
    "rus = RandomOverSampler(sampling_strategy= 'minority',random_state=42) #\n",
    "# fit and apply the transform\n",
    "X_over, y_over = rus.fit_resample(np.array(list(X_train)),np.array(list(y_train)) )\n",
    "\n",
    "X_train = X_over.copy()\n",
    "y_train = y_over.copy()\n",
    "\n",
    "# y_train = y_train.sort_values()\n",
    "# SORTED_INDICES= list(y_train.sort_values().index)\n",
    "# X_train = X_train.loc[SORTED_INDICES]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform( np.array(list(X_train)) )\n",
    "X_test = scaler.fit_transform( np.array(list(X_test)) )\n",
    "\n",
    "model_NB = ComplementNB()\n",
    "model_NB.fit(X_train,y_train)\n",
    "pred_NB = model_NB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true= y_test, y_pred= pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14783476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6eb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947de1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2951ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f51871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014af06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2dabada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: text_span, dtype: object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing examples\n",
    "\n",
    "## Points to be noted:\n",
    "## 892 index: Text : #thedignityproject text_span = thedignityproject \n",
    "## -> To capture the essence of hashtag. I will replace #abc -> hashtagfea abc & get the hashtagfea into textspan so that the embedding of hashtag is present \n",
    "\n",
    "## Text : selected. text_span = selected\n",
    "## -> To tackle this replaced '.' with ' . '\n",
    "\n",
    "## hashtag example\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: ' '.join(x)).str.find('thedignityproject') != -1),'text' ].to_list()\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: ' '.join(x)).str.find('thedignityproject') != -1),'text_split' ].to_list()\n",
    "\n",
    "## merged hashtag example\n",
    "pcl_cat_df.loc[ pcl_cat_df['text_split'].apply(lambda x: 'hashtagfea' in x),'text' ].to_list()\n",
    "\n",
    "## irregular clip - futur example\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: 'futur' in x)),'text_span' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
