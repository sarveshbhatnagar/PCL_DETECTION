{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c64ec2",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "843dc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools, re\n",
    "from collections import Counter \n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB, ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import  recall_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c65fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task2():\n",
    "    temp = open('dontpatronizeme_categories.tsv')\n",
    "    check = temp.read()\n",
    "    tag2id = {'Unbalanced_power_relations':0,'Shallow_solution':1,'Presupposition':2,'Authority_voice':3,'Metaphors':4,\n",
    "              'Compassion':5,'The_poorer_the_merrier':6}\n",
    "    ls = check.split('\\n')[4:]\n",
    "    df = pd.DataFrame([line.split('\\t')  for line in ls if len(line)>0])\n",
    "    df.columns = ['par_id', 'art_id','text' ,'keyword', 'country','start','finish','text_span','label','num_annotators' ]\n",
    "    df['labelid'] = df['label'].map(tag2id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556c520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_cat_df = load_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "43b1714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.350725\n",
       "5    0.247101\n",
       "2    0.107246\n",
       "3    0.103261\n",
       "4    0.090580\n",
       "1    0.082246\n",
       "6    0.018841\n",
       "Name: labelid, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcl_cat_df.labelid.value_counts()/pcl_cat_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e7f31",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa3f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_words(text):\n",
    "        \"\"\"\n",
    "        Removes contractions from the text.\n",
    "        \"\"\"\n",
    "        return contractions.fix(text)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Should return a list of words\n",
    "    \"\"\"\n",
    "    text = contract_words(text)\n",
    "    text = text.lower()\n",
    "#     text = text.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "    text = text.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\".\",\" .\") ## added by PAVAN\n",
    "    \n",
    "    ## To capture multiple # feature -- added by PAVAN\n",
    "    if re.search(r'[a-z]+\\#',text) :\n",
    "        tmp_ls = text.split()\n",
    "        text = ' '.join( [re.sub(pattern=r'\\#',repl=' #',string=str(i)) if re.search(r'[a-z]+\\#',str(i)) else i for i in tmp_ls])\n",
    "        \n",
    "    ## To capture # feature -- added by PAVAN\n",
    "    if re.search(r'\\#[a-z]+',text) :\n",
    "        tmp_ls = text.split()\n",
    "        text = ' '.join( [re.sub(pattern=r'\\#',repl='hashtagfea ',string=str(i)) if re.search(r'\\#[a-z]+',str(i)) else i for i in tmp_ls])\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b016e71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hashtagfea', 'thedignityproject']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('#thedignityproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bcb960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hashtagfea', 'ichh', 'hashtagfea', 'thedignityproject']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('#ichh#thedignityproject')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e619f0a",
   "metadata": {},
   "source": [
    "## FEATURE GENERATION - word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c866c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModelTrainer:\n",
    "    \"\"\"\n",
    "    Word2Vec Features. Basic Process: call train, call load.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences=[], path=\"word2vec.wordvectors\"):\n",
    "        \"\"\"\n",
    "        params: \n",
    "        sentences: list of sentences (default: [])\n",
    "        NOTE sentences should be like this \n",
    "        [[\"i\", \"am\", \"ready\"],[\"some\",\"other\",\"sentence\"]]\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.path = path\n",
    "    \n",
    "    ## UPDATED version -- PAVAN\n",
    "    def train(self, size=100, window=7):\n",
    "        \"\"\"\n",
    "        Trains the model on sentences\n",
    "        params: \n",
    "        size: size of the vector\n",
    "        window: window size\n",
    "        \"\"\"\n",
    "        \n",
    "        model = Word2Vec(sentences=self.sentences, vector_size=size,\n",
    "                         window=window, min_count=1, workers=4)\n",
    "        model.save(\"word2vec_categories.model\")\n",
    "        \n",
    "        model.build_vocab(corpus_iterable=self.sentences, update=True)\n",
    "        \n",
    "        model.train(corpus_iterable=self.sentences,\n",
    "                        total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "#         model.train(sentences=self.sentences,\n",
    "#                     total_examples=model.corpus_count, epochs=30)\n",
    "        word_vectors = model.wv\n",
    "        word_vectors.save(self.path)\n",
    "        return model, word_vectors\n",
    "\n",
    "    def load_trained(self, path=\"\"):\n",
    "        \"\"\"\n",
    "        Loads the trained model\n",
    "        \"\"\"\n",
    "        if(path != \"\"):\n",
    "            load_path = path\n",
    "        else:\n",
    "            load_path = self.path\n",
    "        wv = KeyedVectors.load(load_path, mmap='r')\n",
    "        return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features\n",
    "def get_bigrams(text_split):\n",
    "    \"\"\"\n",
    "    :param text_split: split text (ref preprocessing).\n",
    "    :return: list of bigrams\n",
    "    \"\"\"\n",
    "    bigrams = [[text_split[i], text_split[i+1]]\n",
    "               for i in range(len(text_split)-1)]\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "def add_vectors(list_of_words, wv):\n",
    "    \"\"\"\n",
    "    Returns sum of word vectors, use on text split (ref preprocessing)\n",
    "    params:\n",
    "    list_of_words: list of words\n",
    "    wv: word2vec model\n",
    "    \"\"\"\n",
    "    return sum([wv[word] for word in list_of_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a394bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gensim.models.word2vec.Word2Vec at 0x1e1575656d0>,\n",
       " <gensim.models.keyedvectors.KeyedVectors at 0x1e157565af0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For creating word2vec model & embeddings -> I am using 'text' column. Since it has all the data\n",
    "pcl_cat_df['text_split'] =pcl_cat_df['text'].apply(preprocess_text)\n",
    "\n",
    "unique_sentences_ls =  pcl_cat_df['text_split'].to_list() \n",
    "# # Train WordVectors. Only run once.\n",
    "Word2VecModelTrainer(sentences=unique_sentences_ls, path=\"word2vec_categories.wordvectors\").train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24fc6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained word vectors.\n",
    "wv = Word2VecModelTrainer().load_trained(\"word2vec_categories.wordvectors\")\n",
    "\n",
    "pcl_cat_df['text_span_split'] =pcl_cat_df['text_span'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "## Get the treatment dictionary\n",
    "def get_treatment_map(TEXT_SPAN_SPLIT_SERIES, TEXT_SPLIT_SERIES, EMBED):\n",
    "    def custom_key(str):\n",
    "        return -len(str), str.lower()\n",
    "    text_span_token_ls = list(set( list( itertools.chain.from_iterable(TEXT_SPAN_SPLIT_SERIES) ))) #pcl_cat_df.text_span_split\n",
    "    text_token_ls = list(set( list( itertools.chain.from_iterable( TEXT_SPLIT_SERIES ) ))) #pcl_cat_df.text_split\n",
    "    TOBE_TREATED = [token for token in text_span_token_ls if token not in text_token_ls]\n",
    "\n",
    "    sorted_token_ls = sorted(sorted(EMBED.key_to_index.keys() ), key=custom_key)\n",
    "    EMBED_sorted = {i:EMBED[i] for i in sorted_token_ls}\n",
    "    TREATING_TOKENS_MAP ={i:j for i in TOBE_TREATED for j in EMBED_sorted if re.match(pattern=i,string=j) }\n",
    "    return TREATING_TOKENS_MAP\n",
    "\n",
    "TREATING_TOKENS_MAP = get_treatment_map(pcl_cat_df.text_span_split,pcl_cat_df.text_split , wv)\n",
    "\n",
    "## Applying the Treatment on \"text_span_split\" column\n",
    "pcl_cat_df['text_span_split'] = pcl_cat_df['text_span_split'].apply(lambda x: [TREATING_TOKENS_MAP[i] if i in TREATING_TOKENS_MAP else i for i in x])\n",
    "\n",
    "pcl_cat_df['embeddings'] = pcl_cat_df['text_span_split'].apply(add_vectors, wv=wv) #basic_features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58415c0",
   "metadata": {},
   "source": [
    "## MODEL VERSION 1 - word2vec- NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0de92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use 'labelid' column for using CategoricalNB\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pcl_cat_df['embeddings'], pcl_cat_df['labelid'], stratify=pcl_cat_df['labelid'], test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# # define oversampling strategy\n",
    "# rus = RandomUnderSampler(sampling_strategy= 'majority',random_state=42) #\n",
    "# # fit and apply the transform\n",
    "# X_over, y_over = rus.fit_resample(np.array(list(X_train)),np.array(list(y_train)) )\n",
    "\n",
    "# X_train = X_over.copy()\n",
    "# y_train = y_over.copy()\n",
    "\n",
    "# y_train = y_train.sort_values()\n",
    "# SORTED_INDICES= list(y_train.sort_values().index)\n",
    "# X_train = X_train.loc[SORTED_INDICES]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform( np.array(list(X_train)) )\n",
    "X_test = scaler.fit_transform( np.array(list(X_test)) )\n",
    "\n",
    "model_NB = ComplementNB()\n",
    "model_NB.fit(X_train,y_train)\n",
    "pred_NB = model_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c688ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.64       194\n",
      "           1       0.00      0.00      0.00        46\n",
      "           2       0.14      0.08      0.10        59\n",
      "           3       0.16      0.40      0.22        57\n",
      "           4       0.20      0.04      0.07        50\n",
      "           5       0.48      0.53      0.50       136\n",
      "           6       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.41       552\n",
      "   macro avg       0.23      0.24      0.22       552\n",
      "weighted avg       0.38      0.41      0.39       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true= y_test, y_pred= pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651b057",
   "metadata": {},
   "source": [
    "### Feature Generation: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bedc3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pcl_cat_df.text_split.apply(lambda x: ' '.join(x) ).drop_duplicates().to_list()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(pcl_cat_df.text_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e9797af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_textspan_tfidf = pd.DataFrame(X.toarray(),columns= vectorizer.get_feature_names())\n",
    "pca = PCA(n_components=100,random_state=123)\n",
    "\n",
    "pcl_cat_df['tfidf_embeddings'] = list(  list( pca.fit_transform(pcl_textspan_tfidf) )) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66952fb",
   "metadata": {},
   "source": [
    "## MODEL VERSION 2 - tfidf- NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0972de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       194\n",
      "           1       0.26      0.11      0.15        46\n",
      "           2       0.26      0.10      0.15        59\n",
      "           3       0.19      0.07      0.10        57\n",
      "           4       0.26      0.36      0.30        50\n",
      "           5       0.52      0.38      0.43       136\n",
      "           6       0.02      0.20      0.03        10\n",
      "\n",
      "    accuracy                           0.36       552\n",
      "   macro avg       0.29      0.26      0.25       552\n",
      "weighted avg       0.41      0.36      0.37       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pcl_cat_df['tfidf_embeddings'], pcl_cat_df['labelid'], stratify=pcl_cat_df['labelid'], test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# define oversampling strategy\n",
    "rus = RandomOverSampler(sampling_strategy= 'minority',random_state=42) #\n",
    "# fit and apply the transform\n",
    "X_over, y_over = rus.fit_resample(np.array(list(X_train)),np.array(list(y_train)) )\n",
    "\n",
    "X_train = X_over.copy()\n",
    "y_train = y_over.copy()\n",
    "\n",
    "# y_train = y_train.sort_values()\n",
    "# SORTED_INDICES= list(y_train.sort_values().index)\n",
    "# X_train = X_train.loc[SORTED_INDICES]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform( np.array(list(X_train)) )\n",
    "X_test = scaler.fit_transform( np.array(list(X_test)) )\n",
    "\n",
    "model_NB = ComplementNB()\n",
    "model_NB.fit(X_train,y_train)\n",
    "pred_NB = model_NB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true= y_test, y_pred= pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab65bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcdc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17b3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658d4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d11e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "da8763e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: text_span, dtype: object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing examples\n",
    "\n",
    "## Points to be noted:\n",
    "## 892 index: Text : #thedignityproject text_span = thedignityproject \n",
    "## -> To capture the essence of hashtag. I will replace #abc -> hashtagfea abc & get the hashtagfea into textspan so that the embedding of hashtag is present \n",
    "\n",
    "## Text : selected. text_span = selected\n",
    "## -> To tackle this replaced '.' with ' . '\n",
    "\n",
    "## hashtag example\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: ' '.join(x)).str.find('thedignityproject') != -1),'text' ].to_list()\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: ' '.join(x)).str.find('thedignityproject') != -1),'text_split' ].to_list()\n",
    "\n",
    "## merged hashtag example\n",
    "pcl_cat_df.loc[ pcl_cat_df['text_split'].apply(lambda x: 'hashtagfea' in x),'text' ].to_list()\n",
    "\n",
    "## irregular clip - futur example\n",
    "pcl_cat_df.loc[ (pcl_cat_df['text_span_split'].apply(lambda x: 'futur' in x)),'text_span' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21490a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
